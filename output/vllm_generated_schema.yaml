parameters:
  allowed_local_media_path:
    data_type: bool
    description: ALLOWED_LOCAL_MEDIA_PATH Allowing API requests to read local images
      or videos from directories specified by the server file system. This is a security
      risk. Should only be enabled in trusted environments.
    type: boolean
  async_scheduling:
    data_type: bool
    description: '--no-async-scheduling EXPERIMENTAL: If set to True, perform async
      scheduling. This may help reduce the CPU overheads, leading to better latency
      and throughput. However, async scheduling is currently not supported with some
      features such as structured outputs, speculative decoding, and pipeline parallelism.'
    type: boolean
  block_size:
    data_type: str
    description: '{1,8,16,32,64,128} Size of a contiguous cache block in number of
      tokens. This is ignored on neuron devices and set to `--max- model-len`. On
      CUDA devices, only block sizes up to 32 are supported. On HPU devices, block
      size defaults to 128. This config has no static default. If left unspecified
      by the user, it will be set in `Platform.check_and_update_config()` based on
      the current platform.'
    options:
    - '1'
    - '8'
    - '16'
    - '32'
    - '64'
    - '128'
    type: list
  calculate_kv_scales:
    data_type: bool
    description: --no-calculate-kv-scales This enables dynamic calculation of `k_scale`
      and `v_scale` when kv_cache_dtype is fp8. If `False`, the scales will be loaded
      from the model checkpoint if available. Otherwise, the scales will default to
      1.0.
    type: boolean
  code_revision:
    data_type: str
    description: CODE_REVISION The specific revision to use for the model code on
      the Hugging Face Hub. It can be a branch name, a tag name, or a commit id. If
      unspecified, will use the default version.
    type: string
  config_format:
    data_type: str
    description: '{auto,hf,mistral} The format of the model config to load: - "auto"
      will try to load the config in hf format if available else it will try to load
      in mistral format. - "hf" will load the config in hf format. - "mistral" will
      load the config in mistral format.'
    options:
    - auto
    - hf
    - mistral
    type: list
  convert:
    data_type: str
    description: '{auto,classify,embed,none,reward} Convert the model using adapters
      defined in [vllm.model_executor.models.adapters][]. The most common use case
      is to adapt a text generation model to be used for pooling tasks.'
    options:
    - auto
    - classify
    - embed
    - none
    - reward
    type: list
  cpu_offload_gb:
    data_type: int
    description: CPU_OFFLOAD_GB The space in GiB to offload to CPU, per GPU. Default
      is 0, which means no offloading. Intuitively, this argument can be seen as a
      virtual way to increase the GPU memory size. For example, if you have one 24
      GB GPU and set this to 10, virtually you can think of it as a 34 GB GPU. Then
      you can load a 13B model with BF16 weight, which requires at least 26GB GPU
      memory. Note that this requires fast CPU-GPU interconnect, as part of the model
      is loaded from CPU memory to GPU memory on the fly in each model forward pass.
    max: 1000
    min: 0
    step: 1
    type: range
  cuda_graph_sizes:
    data_type: str
    description: 'CUDA_GRAPH_SIZES [CUDA_GRAPH_SIZES ...] Cuda graph capture sizes
      1. if none provided, then default set to [min(max_num_seqs * 2, 512)] 2. if
      one value is provided, then the capture list would follow the pattern: [1, 2,
      4] + [i for i in range(8, cuda_graph_sizes + 1, 8)] 3. more than one value (e.g.
      1 2 128) is provided, then the capture list will follow the provided list.'
    type: list
  data_parallel_address:
    data_type: str
    description: DATA_PARALLEL_ADDRESS, -dpa DATA_PARALLEL_ADDRESS Address of data
      parallel cluster head-node.
    type: list
  data_parallel_backend:
    data_type: str
    description: DATA_PARALLEL_BACKEND, -dpb DATA_PARALLEL_BACKEND Backend for data
      parallel, either "mp" or "ray".
    type: string
  data_parallel_hybrid_lb:
    data_type: bool
    description: --no-data-parallel-hybrid-lb Whether to use "hybrid" DP LB mode.
      Applies only to online serving and when data_parallel_size > 0. Enables running
      an AsyncLLM and API server on a "per- node" basis where vLLM load balances between
      local data parallel ranks, but an external LB balances between vLLM nodes/replicas.
      Set explicitly in conjunction with --data-parallel-start-rank.
    type: boolean
  data_parallel_rank:
    data_type: bool
    description: DATA_PARALLEL_RANK, -dpn DATA_PARALLEL_RANK Data parallel rank of
      this instance. When set, enables external load balancer mode.
    type: boolean
  data_parallel_rpc_port:
    data_type: str
    description: DATA_PARALLEL_RPC_PORT, -dpp DATA_PARALLEL_RPC_PORT Port for data
      parallel RPC communication.
    type: string
  data_parallel_size:
    data_type: int
    description: DATA_PARALLEL_SIZE, -dp DATA_PARALLEL_SIZE Number of data parallel
      groups. MoE layers will be sharded according to the product of the tensor parallel
      size and data parallel size.
    max: 1001
    min: 0
    step: 1
    type: range
  data_parallel_size_local:
    data_type: int
    description: DATA_PARALLEL_SIZE_LOCAL, -dpl DATA_PARALLEL_SIZE_LOCAL Number of
      data parallel replicas to run on this node.
    type: range
  data_parallel_start_rank:
    data_type: str
    description: DATA_PARALLEL_START_RANK, -dpr DATA_PARALLEL_START_RANK Starting
      data parallel rank for secondary nodes.
    type: string
  disable_async_output_proc:
    data_type: bool
    description: Disable async output processing. This may result in lower performance.
    type: boolean
  disable_cascade_attn:
    data_type: bool
    description: --no-disable-cascade-attn Disable cascade attention for V1. While
      cascade attention does not change the mathematical correctness, disabling it
      could be useful for preventing potential numerical issues. Note that even if
      this is set to False, cascade attention will be only used when the heuristic
      tells that it's beneficial.
    type: boolean
  disable_chunked_mm_input:
    data_type: bool
    description: --no-disable-chunked-mm-input If set to true and chunked prefill
      is enabled, we do not want to partially schedule a multimodal item. Only used
      in V1 This ensures that if a request has a mixed prompt (like text tokens TTTT
      followed by image tokens IIIIIIIIII) where only some image tokens can be scheduled
      (like TTTTIIIII, leaving IIIII), it will be scheduled as TTTT in one step and
      IIIIIIIIII in the next.
    type: boolean
  disable_custom_all_reduce:
    data_type: bool
    description: --no-disable-custom-all-reduce Disable the custom all-reduce kernel
      and fall back to NCCL.
    type: boolean
  disable_hybrid_kv_cache_manager:
    data_type: bool
    description: --no-disable-hybrid-kv-cache-manager If set to True, KV cache manager
      will allocate the same size of KV cache for all attention layers even if there
      are multiple type of attention layers like full attention and sliding window
      attention.
    type: boolean
  disable_sliding_window:
    data_type: bool
    description: --no-disable-sliding-window Whether to disable sliding window. If
      True, we will disable the sliding window functionality of the model, capping
      to sliding window size. If the model does not support sliding window, this argument
      is ignored.
    type: boolean
  distributed_executor_backend:
    data_type: str
    description: '{external_launcher,mp,ray,uni,None} Backend to use for distributed
      model workers, either "ray" or "mp" (multiprocessing). If the product of pipeline_parallel_size
      and tensor_parallel_size is less than or equal to the number of GPUs available,
      "mp" will be used to keep processing on a single host. Otherwise, this will
      default to "ray" if Ray is installed and fail otherwise. Note that tpu only
      support Ray for distributed inference.'
    options:
    - external_launcher
    - mp
    - ray
    - uni
    - None
    type: list
  dtype:
    data_type: str
    description: '{auto,bfloat16,float,float16,float32,half} Data type for model weights
      and activations: - "auto" will use FP16 precision for FP32 and FP16 models,
      and BF16 precision for BF16 models. - "half" for FP16. Recommended for AWQ quantization.
      - "float16" is the same as "half". - "bfloat16" for a balance between precision
      and range. - "float" is shorthand for FP32 precision. - "float32" for FP32 precision.'
    options:
    - auto
    - bfloat16
    - float
    - float16
    - float32
    - half
    type: list
  enable_chunked_prefill:
    data_type: bool
    description: --no-enable-chunked-prefill If True, prefill requests can be chunked
      based on the remaining max_num_batched_tokens.
    type: boolean
  enable_eplb:
    data_type: bool
    description: --no-enable-eplb Enable expert parallelism load balancing for MoE
      layers.
    type: boolean
  enable_expert_parallel:
    data_type: bool
    description: --no-enable-expert-parallel Use expert parallelism instead of tensor
      parallelism for MoE layers.
    type: boolean
  enable_multimodal_encoder_data_parallel:
    data_type: bool
    description: --no-enable-multimodal-encoder-data-parallel Use data parallelism
      instead of tensor parallelism for vision encoder. Only support LLama4 for now
    type: boolean
  enable_prefix_caching:
    data_type: bool
    description: --no-enable-prefix-caching Whether to enable prefix caching. Disabled
      by default for V0. Enabled by default for V1.
    type: boolean
  enable_prompt_embeds:
    data_type: bool
    description: --no-enable-prompt-embeds If `True`, enables passing text embeddings
      as inputs via the `prompt_embeds` key. Note that enabling this will double the
      time required for graph compilation.
    type: boolean
  enable_sleep_mode:
    data_type: bool
    description: --no-enable-sleep-mode Enable sleep mode for the engine (only cuda
      platform is supported).
    type: boolean
  enforce_eager:
    data_type: bool
    description: --no-enforce-eager Whether to always use eager-mode PyTorch. If True,
      we will disable CUDA graph and always execute the model in eager mode. If False,
      we will use CUDA graph and eager execution in hybrid for maximal performance
      and flexibility.
    type: boolean
  eplb_log_balancedness:
    data_type: bool
    description: --no-eplb-log-balancedness Log the balancedness each step of expert
      parallelism. This is turned off by default since it will cause communication
      overhead.
    type: boolean
  eplb_step_interval:
    data_type: int
    description: EPLB_STEP_INTERVAL Interval for rearranging experts in expert parallelism.
      Note that if this is greater than the EPLB window size, only the metrics of
      the last `eplb_window_size` steps will be used for rearranging experts.
    max: 4000
    min: 2900
    step: 1
    type: range
  eplb_window_size:
    data_type: int
    description: EPLB_WINDOW_SIZE Window size for expert load recording.
    max: 2000
    min: 900
    step: 1
    type: range
  generation_config:
    data_type: str
    description: GENERATION_CONFIG The folder path to the generation config. Defaults
      to `"auto"`, the generation config will be loaded from model path. If set to
      `"vllm"`, no generation config is loaded, vLLM defaults will be used. If set
      to a folder path, the generation config will be loaded from the specified folder
      path. If `max_new_tokens` is specified in generation config, then it sets a
      server- wide limit on the number of output tokens for all requests.
    type: string
  gpu_memory_utilization:
    data_type: float
    description: GPU_MEMORY_UTILIZATION The fraction of GPU memory to be used for
      the model executor, which can range from 0 to 1. For example, a value of 0.5
      would imply 50% GPU memory utilization. If unspecified, will use the default
      value of 0.9. This is a per-instance limit, and only applies to the current
      vLLM instance. It does not matter if you have another vLLM instance running
      on the same GPU. For example, if you have two vLLM instances running on the
      same GPU, you can set the GPU memory utilization to 0.5 for each instance.
    max: 1.0
    min: 0.0
    step: 0.01
    type: range
  hf_config_path:
    data_type: str
    description: HF_CONFIG_PATH Name or path of the Hugging Face config to use. If
      unspecified, model name or path will be used.
    type: string
  hf_overrides:
    data_type: str
    description: HF_OVERRIDES If a dictionary, contains arguments to be forwarded
      to the Hugging Face config. If a callable, it is called to update the HuggingFace
      config.
    type: list
  hf_token:
    data_type: str
    description: '[HF_TOKEN] The token to use as HTTP bearer authorization for remote
      files . If `True`, will use the token generated when running `huggingface-cli
      login` (stored in `~/.huggingface`).'
    type: list
  kv_cache_dtype:
    data_type: str
    description: '{auto,fp8,fp8_e4m3,fp8_e5m2,fp8_inc} Data type for kv cache storage.
      If "auto", will use model data type. CUDA 11.8+ supports fp8 (=fp8_e4m3) and
      fp8_e5m2. ROCm (AMD GPU) supports fp8 (=fp8_e4m3). Intel Gaudi (HPU) supports
      fp8 (using fp8_inc).'
    options:
    - auto
    - fp8
    - fp8_e4m3
    - fp8_e5m2
    - fp8_inc
    type: list
  kv_sharing_fast_prefill:
    data_type: bool
    description: --no-kv-sharing-fast-prefill This feature is work in progress and
      no prefill optimization takes place with this flag enabled currently. In some
      KV sharing setups, e.g. YOCO (https://arxiv.org/abs/2405.05254), some layers
      can skip tokens corresponding to prefill. This flag enables attention metadata
      for eligible layers to be overriden with metadata necessary for implementating
      this optimization in some models (e.g. Gemma3n)
    type: boolean
  logits_processor_pattern:
    data_type: str
    description: LOGITS_PROCESSOR_PATTERN Optional regex pattern specifying valid
      logits processor qualified names that can be passed with the `logits_processors`
      extra completion argument. Defaults to `None`, which allows no processors.
    type: string
  logits_processors:
    data_type: str
    description: LOGITS_PROCESSORS [LOGITS_PROCESSORS ...] One or more logits processors'
      fully-qualified class names or class definitions
    type: list
  logprobs_mode:
    data_type: str
    description: '{processed_logits,processed_logprobs,raw_logits,raw_logprobs} Indicates
      the content returned in the logprobs and prompt_logprobs. Supported mode: 1)
      raw_logprobs, 2) processed_logprobs, 3) raw_logits, 4) processed_logits. Raw
      means the values before applying logit processors, like bad words. Processed
      means the values after applying such processors.'
    options:
    - processed_logits
    - processed_logprobs
    - raw_logits
    - raw_logprobs
    type: list
  long_prefill_token_threshold:
    data_type: int
    description: LONG_PREFILL_TOKEN_THRESHOLD For chunked prefill, a request is considered
      long if the prompt is longer than this number of tokens.
    max: 1000
    min: 0
    step: 1
    type: range
  mamba_cache_dtype:
    data_type: str
    description: '{auto,float32} The data type to use for the Mamba cache (both the
      conv as well as the ssm state). If set to ''auto'', the data type will be inferred
      from the model config.'
    options:
    - auto
    - float32
    type: list
  mamba_ssm_cache_dtype:
    data_type: str
    description: '{auto,float32} The data type to use for the Mamba cache (ssm state
      only, conv state will still be controlled by mamba_cache_dtype). If set to ''auto'',
      the data type for the ssm state will be determined by mamba_cache_dtype.'
    options:
    - auto
    - float32
    type: list
  max_logprobs:
    data_type: str
    description: MAX_LOGPROBS Maximum number of log probabilities to return when `logprobs`
      is specified in `SamplingParams`. The default value comes the default for the
      OpenAI Chat Completions API. -1 means no cap, i.e. all (output_length * vocab_size)
      logprobs are allowed to be returned and it may cause OOM.
    type: list
  max_long_partial_prefills:
    data_type: str
    description: MAX_LONG_PARTIAL_PREFILLS For chunked prefill, the maximum number
      of prompts longer than long_prefill_token_threshold that will be prefilled concurrently.
      Setting this less than max_num_partial_prefills will allow shorter prompts to
      jump the queue in front of longer prompts in some cases, improving latency.
    type: list
  max_model_len:
    data_type: str
    description: 'MAX_MODEL_LEN Model context length (prompt and output). If unspecified,
      will be automatically derived from the model config. When passing via `--max-model-len`,
      supports k/m/g/K/M/G in human-readable format. Examples: - 1k -> 1000 - 1K ->
      1024 - 25.6k -> 25,600'
    type: string
  max_num_batched_tokens:
    data_type: str
    description: MAX_NUM_BATCHED_TOKENS Maximum number of tokens to be processed in
      a single iteration. This config has no static default. If left unspecified by
      the user, it will be set in `EngineArgs.create_engine_config` based on the usage
      context.
    type: list
  max_num_partial_prefills:
    data_type: str
    description: MAX_NUM_PARTIAL_PREFILLS For chunked prefill, the maximum number
      of sequences that can be partially prefilled concurrently.
    type: list
  max_num_seqs:
    data_type: str
    description: MAX_NUM_SEQS Maximum number of sequences to be processed in a single
      iteration. This config has no static default. If left unspecified by the user,
      it will be set in `EngineArgs.create_engine_config` based on the usage context.
    type: list
  max_parallel_loading_workers:
    data_type: str
    description: MAX_PARALLEL_LOADING_WORKERS Maximum number of parallel loading workers
      when loading model sequentially in multiple batches. To avoid RAM OOM when using
      tensor parallel and large models.
    type: list
  max_seq_len_to_capture:
    data_type: int
    description: MAX_SEQ_LEN_TO_CAPTURE Maximum sequence len covered by CUDA graphs.
      When a sequence has context length larger than this, we fall back to eager mode.
      Additionally for encoder-decoder models, if the sequence length of the encoder
      input is larger than this, we fall back to the eager mode.
    max: 9192
    min: 8092
    step: 1
    type: range
  model_impl:
    data_type: str
    description: '{auto,vllm,transformers} Which implementation of the model to use:
      - "auto" will try to use the vLLM implementation, if it exists, and fall back
      to the Transformers implementation if no vLLM implementation is available. -
      "vllm" will use the vLLM model implementation. - "transformers" will use the
      Transformers model implementation.'
    options:
    - auto
    - vllm
    - transformers
    type: list
  num_gpu_blocks_override:
    data_type: int
    description: NUM_GPU_BLOCKS_OVERRIDE Number of GPU blocks to use. This overrides
      the profiled `num_gpu_blocks` if specified. Does nothing if `None`. Used for
      testing preemption.
    type: range
  num_lookahead_slots:
    data_type: bool
    description: 'NUM_LOOKAHEAD_SLOTS The number of slots to allocate per sequence
      per step, beyond the known token ids. This is used in speculative decoding to
      store KV activations of tokens which may or may not be accepted. NOTE: This
      will be replaced by speculative config in the future; it is present to enable
      correctness tests until then.'
    type: boolean
  num_redundant_experts:
    data_type: str
    description: NUM_REDUNDANT_EXPERTS Number of redundant experts to use for expert
      parallelism.
    type: list
  override_attention_dtype:
    data_type: str
    description: OVERRIDE_ATTENTION_DTYPE Override dtype for attention
    type: string
  override_generation_config:
    data_type: str
    description: 'OVERRIDE_GENERATION_CONFIG Overrides or sets generation config.
      e.g. `{"temperature": 0.5}`. If used with `--generation- config auto`, the override
      parameters will be merged with the default config from the model. If used with
      `--generation-config vllm`, only the override parameters are used. Should either
      be a valid JSON string or JSON keys passed individually.'
    options:
    - '"temperature": 0.5'
    type: list
  override_neuron_config:
    data_type: str
    description: 'OVERRIDE_NEURON_CONFIG Initialize non-default neuron config or override
      default neuron config that are specific to Neuron devices, this argument will
      be used to configure the neuron config that can not be gathered from the vllm
      arguments. e.g. `{"cast_logits_dtype": "bfloat16"}`. Should either be a valid
      JSON string or JSON keys passed individually.'
    options:
    - '"cast_logits_dtype": "bfloat16"'
    type: list
  override_pooler_config:
    data_type: str
    description: 'OVERRIDE_POOLER_CONFIG Initialize non-default pooling config or
      override default pooling config for the pooling model. e.g. `{"pooling_type":
      "mean", "normalize": false}`.'
    options:
    - '"pooling_type": "mean"'
    - '"normalize": false'
    type: list
  pipeline_parallel_size:
    data_type: int
    description: PIPELINE_PARALLEL_SIZE, -pp PIPELINE_PARALLEL_SIZE Number of pipeline
      parallel groups.
    max: 1001
    min: 0
    step: 1
    type: range
  preemption_mode:
    data_type: str
    description: '{recompute,swap,None} Whether to perform preemption by swapping
      or recomputation. If not specified, we determine the mode as follows: We use
      recomputation by default since it incurs lower overhead than swapping. However,
      when the sequence group has multiple sequences (e.g., beam search), recomputation
      is not currently supported. In such a case, we use swapping instead.'
    options:
    - recompute
    - swap
    - None
    type: list
  prefix_caching_hash_algo:
    data_type: str
    description: '{builtin,sha256,sha256_cbor_64bit} Set the hash algorithm for prefix
      caching: - "builtin" is Python''s built-in hash. - "sha256" is collision resistant
      but with certain overheads. This option uses Pickle for object serialization
      before hashing. - "sha256_cbor_64bit" provides a reproducible, cross- language
      compatible hash. It serializes objects using canonical CBOR and hashes them
      with SHA-256. The resulting hash consists of the lower 64 bits of the SHA-256
      digest.'
    options:
    - builtin
    - sha256
    - sha256_cbor_64bit
    type: list
  quantization:
    data_type: str
    description: QUANTIZATION, -q QUANTIZATION Method used to quantize the weights.
      If `None`, we first check the `quantization_config` attribute in the model config
      file. If that is `None`, we assume the model weights are not quantized and use
      `dtype` to determine the data type of the weights.
    type: string
  ray_workers_use_nsight:
    data_type: bool
    description: --no-ray-workers-use-nsight Whether to profile Ray workers with nsight,
      see https://docs.ray.io/en/latest/ray-observability/user- guides/profiling.html#profiling-nsight-profiler.
    type: boolean
  revision:
    data_type: str
    description: REVISION   The specific model version to use. It can be a branch
      name, a tag name, or a commit id. If unspecified, will use the default version.
    type: string
  rope_scaling:
    data_type: str
    description: ROPE_SCALING RoPE scaling configuration. For example, `{"rope_type":"dynamic","factor":2.0}`.
      Should either be a valid JSON string or JSON keys passed individually.
    options:
    - '"rope_type":"dynamic"'
    - '"factor":2.0'
    type: list
  rope_theta:
    data_type: str
    description: ROPE_THETA RoPE theta. Use with `rope_scaling`. In some cases, changing
      the RoPE theta improves the performance of the scaled model.
    type: string
  runner:
    data_type: str
    description: '{auto,draft,generate,pooling} The type of model runner to use. Each
      vLLM instance only supports one model runner, even if the same model can be
      used for multiple types.'
    options:
    - auto
    - draft
    - generate
    - pooling
    type: list
  scheduler_cls:
    data_type: str
    description: SCHEDULER_CLS The scheduler class to use. "vllm.core.scheduler.Scheduler"
      is the default scheduler. Can be a class directly or the path to a class of
      form "mod.custom_class".
    type: list
  scheduler_delay_factor:
    data_type: float
    description: SCHEDULER_DELAY_FACTOR Apply a delay (of delay factor multiplied
      by previous prompt latency) before scheduling next prompt.
    max: 1.0
    min: 0.0
    step: 0.01
    type: range
  scheduling_policy:
    data_type: str
    description: '{fcfs,priority} The scheduling policy to use: - "fcfs" means first
      come first served, i.e. requests are handled in order of arrival. - "priority"
      means requests are handled based on given priority (lower value means earlier
      handling) and time of arrival deciding any ties).'
    options:
    - fcfs
    - priority
    type: list
  seed:
    data_type: str
    description: SEED           Random seed for reproducibility. Initialized to None
      in V0, but initialized to 0 in V1.
    type: string
  served_model_name:
    data_type: str
    description: SERVED_MODEL_NAME [SERVED_MODEL_NAME ...] The model name(s) used
      in the API. If multiple names are provided, the server will respond to any of
      the provided names. The model name in the model field of a response will be
      the first name in this list. If not specified, the model name will be the same
      as the `--model` argument. Noted that this name(s) will also be used in `model_name`
      tag content of prometheus metrics, if multiple names provided, metrics tag will
      take the first one.
    type: list
  skip_tokenizer_init:
    data_type: bool
    description: --no-skip-tokenizer-init Skip initialization of tokenizer and detokenizer.
      Expects valid `prompt_token_ids` and `None` for prompt from the input. The generated
      output will contain token ids.
    type: boolean
  swap_space:
    data_type: int
    description: SWAP_SPACE Size of the CPU swap space per GPU (in GiB).
    max: 1004
    min: 0
    step: 1
    type: range
  task:
    data_type: str
    description: '{auto,classify,draft,embed,embedding,generate,reward,score,transcription,None}
      [DEPRECATED] The task to use the model for. If the model supports more than
      one model runner, this is used to select which model runner to run. Note that
      the model may support other tasks using the same model runner.'
    options:
    - auto
    - classify
    - draft
    - embed
    - embedding
    - generate
    - reward
    - score
    - transcription
    - None
    type: list
  tensor_parallel_size:
    data_type: int
    description: TENSOR_PARALLEL_SIZE, -tp TENSOR_PARALLEL_SIZE Number of tensor parallel
      groups.
    max: 1001
    min: 0
    step: 1
    type: range
  tokenizer:
    data_type: str
    description: TOKENIZER Name or path of the Hugging Face tokenizer to use. If unspecified,
      model name or path will be used.
    type: string
  tokenizer_mode:
    data_type: str
    description: '{auto,custom,mistral,slow} Tokenizer mode: - "auto" will use the
      fast tokenizer if available. - "slow" will always use the slow tokenizer. -
      "mistral" will always use the tokenizer from `mistral_common`. - "custom" will
      use --tokenizer to select the preregistered tokenizer.'
    options:
    - auto
    - custom
    - mistral
    - slow
    type: list
  tokenizer_revision:
    data_type: str
    description: TOKENIZER_REVISION The specific revision to use for the tokenizer
      on the Hugging Face Hub. It can be a branch name, a tag name, or a commit id.
      If unspecified, will use the default version.
    type: string
  trust_remote_code:
    data_type: bool
    description: --no-trust-remote-code Trust remote code (e.g., from HuggingFace)
      when downloading the model and tokenizer.
    type: boolean
  worker_cls:
    data_type: str
    description: WORKER_CLS The full name of the worker class to use. If "auto", the
      worker class will be determined based on the platform.
    type: list
  worker_extension_cls:
    data_type: str
    description: WORKER_EXTENSION_CLS The full name of the worker extension class
      to use. The worker extension class is dynamically inherited by the worker class.
      This is used to inject new attributes and methods to the worker class for use
      in collective_rpc calls.
    type: list
