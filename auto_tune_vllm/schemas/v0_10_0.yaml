# vLLM 0.10.0 Parameter Schema with Optimization Ranges and Defaults
# This file contains both optimization ranges and vLLM default values
version: "0.10.0"

parameters:
  # Core performance parameters
  max_num_batched_tokens:
    type: "range"
    data_type: "int"
    min: 1024
    max: 96000
    step: 1024
    default: null  # vLLM auto-calculates based on model and memory
    description: "Maximum tokens processed in a single batch"
    
  gpu_memory_utilization:
    type: "range"
    data_type: "float" 
    min: 0.70
    max: 0.95
    step: 0.01
    default: 0.9  # vLLM default from cacheconfig
    description: "GPU memory usage ratio"
    
  block_size:
    type: "list"
    data_type: "int"
    options: [1, 8, 16, 32, 64, 128]
    default: null  # No static default - determined by platform
    description: "Memory block size for attention"
    
  # Cache and memory parameters
  kv_cache_dtype:
    type: "list"
    data_type: "str"
    options: ["auto", "fp8", "fp8_e4m3", "fp8_e5m2", "fp8_inc"]
    default: "auto"  # vLLM default from cacheconfig
    description: "Key-value cache data type"
    
  # CUDA graph optimization
  cuda_graph_sizes:
    type: "list"
    data_type: "int"
    options: [1000, 2000, 3000, 4000, 5000, 6000, 8000]
    default: []  # vLLM default: empty list (disabled)
    description: "CUDA graph capture sizes"
    
    
  # Prefill optimization
  long_prefill_token_threshold:
    type: "list"
    data_type: "int"
    options: [0, 256, 512, 1024, 2048, 4096, 8192, 11500]
    default: 0  # vLLM default from schedulerconfig
    description: "Threshold for long prefill tokens"
    
  max_seq_len_to_capture:
    type: "list"
    data_type: "int"
    options: [4096, 8192, 12288, 16384]
    default: 8192  # vLLM default from modelconfig
    description: "Maximum sequence length for CUDA graphs"
    
  max_num_partial_prefills:
    type: "list"
    data_type: "int"
    options: [1, 2, 4, 8]
    default: 1  # vLLM default from schedulerconfig
    description: "Maximum partial prefill operations"
    
  # Parallelism parameters
  tensor_parallel_size:
    type: "list"
    data_type: "int"
    options: [1, 2, 4, 8]
    default: 1  # vLLM default from parallelconfig
    description: "Number of tensor parallel groups (GPUs for tensor parallelism)"
    
  pipeline_parallel_size:
    type: "list" 
    data_type: "int"
    options: [1, 2, 4]
    default: 1  # vLLM default from parallelconfig
    description: "Number of pipeline parallel stages"
    
  data_parallel_size:
    type: "list"
    data_type: "int" 
    options: [1, 2, 4]
    default: 1  # vLLM default from parallelconfig
    description: "Number of data parallel replicas"
    
  # Caching features
  enable_prefix_caching:
    type: "boolean" 
    data_type: "bool"
    default: null  # No static default in v0.10.0
    description: "Enable prefix caching"
    
  # Scheduling parameters
  preemption_mode:
    type: "list"
    data_type: "str"
    options: ["recompute", "swap"]
    default: null  # Determined automatically based on use case
    description: "Preemption strategy for memory management"
    
  # Additional cache parameters
  swap_space:
    type: "range"
    data_type: "int"
    min: 0
    max: 16
    step: 1
    default: 4  # vLLM default from cacheconfig
    description: "CPU swap space in GB"
    
  cpu_offload_gb:
    type: "range"
    data_type: "int"
    min: 0
    max: 64
    step: 4
    default: 0  # vLLM default from cacheconfig
    description: "CPU offload memory in GB"

# vLLM CLI defaults for reference (extracted from v0.10.0)
vllm_defaults:
  cacheconfig:
    block_size: null  # No static default - set by platform
    calculate_kv_scales: false
    cpu_offload_gb: 0
    gpu_memory_utilization: 0.9
    kv_cache_dtype: "auto"
    prefix_caching_hash_algo: "builtin"
    swap_space: 4
  
  modelconfig:
    dtype: "auto"
    enforce_eager: false
    max_seq_len_to_capture: 8192
    
  parallelconfig:
    data_parallel_backend: "mp"
    data_parallel_size: 1
    disable_custom_all_reduce: false
    pipeline_parallel_size: 1
    tensor_parallel_size: 1
    
  schedulerconfig:
    async_scheduling: false
    cuda_graph_sizes: []
    enable_chunked_prefill: null  # No static default
    long_prefill_token_threshold: 0
    max_long_partial_prefills: 1
    max_num_batched_tokens: null  # No static default
    max_num_partial_prefills: 1
    max_num_seqs: null  # No static default
    preemption_mode: null  # Determined automatically
    scheduler_delay_factor: 0.0
    scheduling_policy: "fcfs"
