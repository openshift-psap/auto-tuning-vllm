# vLLM 0.10.1.1 Parameter Schema with Optimization Ranges and Defaults
# This file contains both optimization ranges and vLLM default values
version: "0.10.1.1"

parameters:
  # Core performance parameters
  max_num_batched_tokens:
    type: "range"
    data_type: "int"
    min: 1024
    max: 32768
    step: 1024
    default: null  # vLLM auto-calculates based on model and memory
    description: "Maximum tokens processed in a single batch"
    
  gpu_memory_utilization:
    type: "range"
    data_type: "float" 
    min: 0.70
    max: 0.95
    step: 0.01
    default: 0.9  # vLLM default from cacheconfig
    description: "GPU memory usage ratio"
    
  block_size:
    type: "list"
    data_type: "int"
    options: [8, 16, 32, 64, 128]
    default: 16  # Common vLLM default
    description: "Memory block size for attention"
    
  # Cache and memory parameters
  kv_cache_dtype:
    type: "list"
    data_type: "str"
    options: ["auto", "fp8", "fp8_e5m2", "fp8_e4m3"]
    default: "auto"  # vLLM default from cacheconfig
    description: "Key-value cache data type"
    
  # CUDA graph optimization
  cuda_graph_sizes:
    type: "list"
    data_type: "int"
    options: [1000, 2000, 3000, 4000, 5000, 6000, 8000]
    default: []  # vLLM default: empty list (disabled)
    description: "CUDA graph capture sizes"
    
  enable_cuda_graphs:
    type: "boolean"
    data_type: "bool"
    default: false  # Inferred from empty cuda_graph_sizes default
    description: "Enable CUDA graph optimization"
    
  # Prefill optimization
  long_prefill_token_threshold:
    type: "list"
    data_type: "int"
    options: [0, 256, 512, 1024, 2048, 4096, 8192]
    default: 0  # vLLM default from schedulerconfig
    description: "Threshold for long prefill tokens"
    
  max_seq_len_to_capture:
    type: "list"
    data_type: "int"
    options: [4096, 8192, 12288, 16384, 24576, 32768]
    default: 8192  # vLLM default from modelconfig
    description: "Maximum sequence length for CUDA graphs"
    
  max_num_partial_prefills:
    type: "list"
    data_type: "int"
    options: [1, 2, 4, 8]
    default: 1  # vLLM default from schedulerconfig
    description: "Maximum partial prefill operations"
    
  # Parallelism parameters
  tensor_parallel_size:
    type: "list"
    data_type: "int"
    options: [1, 2, 4, 8]
    default: 1  # vLLM default from parallelconfig
    description: "Number of tensor parallel groups (GPUs for tensor parallelism)"
    
  pipeline_parallel_size:
    type: "list" 
    data_type: "int"
    options: [1, 2, 4]
    default: 1  # vLLM default from parallelconfig
    description: "Number of pipeline parallel stages"
    
  data_parallel_size:
    type: "list"
    data_type: "int" 
    options: [1, 2, 4]
    default: 1  # vLLM default from parallelconfig
    description: "Number of data parallel replicas"
    
  # Caching features
  enable_prefix_caching:
    type: "boolean" 
    data_type: "bool"
    default: false  # Not explicitly enabled by default
    description: "Enable prefix caching"
    
  # Additional cache parameters
  swap_space:
    type: "range"
    data_type: "int"
    min: 0
    max: 16
    step: 1
    default: 4  # vLLM default from cacheconfig
    description: "CPU swap space in GB"
    
  cpu_offload_gb:
    type: "range"
    data_type: "int"
    min: 0
    max: 64
    step: 4
    default: 0  # vLLM default from cacheconfig
    description: "CPU offload memory in GB"

# vLLM CLI defaults for reference (extracted from v0.10.1.1)
vllm_defaults:
  cacheconfig:
    calculate_kv_scales: false
    cpu_offload_gb: 0
    gpu_memory_utilization: 0.9
    kv_cache_dtype: "auto"
    swap_space: 4
  
  modelconfig:
    max_seq_len_to_capture: 8192
    
  parallelconfig:
    data_parallel_size: 1
    pipeline_parallel_size: 1
    tensor_parallel_size: 1
    
defaults:    
  schedulerconfig:
    cuda_graph_sizes: []
    long_prefill_token_threshold: 0
    max_num_partial_prefills: 1
