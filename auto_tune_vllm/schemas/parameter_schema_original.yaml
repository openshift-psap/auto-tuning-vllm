# Parameter schema definition for vLLM optimization
# This defines all available parameters and their validation rules

parameters:
  max_num_batched_tokens:
    type: "range"
    data_type: "int"
    min: 1024
    max: 32768
    step: 1024
    description: "Maximum tokens processed in a single batch"
    
  gpu_memory_utilization:
    type: "range"
    data_type: "float" 
    min: 0.70
    max: 0.95
    step: 0.01
    description: "GPU memory usage ratio"
    
  block_size:
    type: "list"
    data_type: "int"
    options: [8, 16, 32, 64, 128]
    description: "Memory block size for attention"
    
  kv_cache_dtype:
    type: "list"
    data_type: "str"
    options: ["auto", "fp8", "fp8_e5m2", "fp8_e4m3"]
    description: "Key-value cache data type"
    
  cuda_graph_sizes:
    type: "range"
    data_type: "int"
    min: 8
    max: 16384
    step: 256
    description: "CUDA graph capture sizes"
    
  long_prefill_token_threshold:
    type: "list"
    data_type: "int"
    options: [0, 256, 512, 1024, 2048, 4096, 8192]
    description: "Threshold for long prefill tokens"
    
  max_seq_len_to_capture:
    type: "list"
    data_type: "int"
    options: [4096, 8192, 12288, 16384, 24576, 32768]
    description: "Maximum sequence length for CUDA graphs"
    
  max_num_partial_prefills:
    type: "list"
    data_type: "int"
    options: [1, 2, 4, 8]
    description: "Maximum partial prefill operations"
    
  compilation_config:
    type: "list"
    data_type: "str"
    options: ['-O0','-O1', '-O2', '-O3']
    description: "Compilation optimization level"
    
  enable_cuda_graphs:
    type: "boolean"
    data_type: "bool"
    description: "Enable CUDA graph optimization"
    
  enable_prefix_caching:
    type: "boolean" 
    data_type: "bool"
    description: "Enable prefix caching"
    
  # Environment variable parameters (list-only)
  VLLM_ATTENTION_BACKEND:
    type: "environment"
    data_type: "str"
    options: ["FLASH_ATTN", "XFORMERS", "ROCM_FLASH"]
    description: "vLLM attention backend implementation"
    
  CUDA_VISIBLE_DEVICES:
    type: "environment"
    data_type: "str"
    options: ["0", "1", "2", "3", "0,1", "0,2", "0,3", "1,2", "1,3", "2,3", "0,1,2", "0,1,3", "0,2,3", "1,2,3", "0,1,2,3"]
    description: "GPU devices visible to CUDA"
    
  VLLM_WORKER_MULTIPROC_METHOD:
    type: "environment"
    data_type: "str"
    options: ["spawn", "fork", "forkserver"]
    description: "Multiprocessing method for vLLM workers"
    
  TOKENIZERS_PARALLELISM:
    type: "environment"
    data_type: "str"
    options: ["true", "false"]
    description: "Enable tokenizer parallelism"
    
  VLLM_CONFIGURE_LOGGING:
    type: "environment"
    data_type: "str"
    options: ["0", "1"]
    description: "Configure vLLM logging (0=disable, 1=enable)"
    